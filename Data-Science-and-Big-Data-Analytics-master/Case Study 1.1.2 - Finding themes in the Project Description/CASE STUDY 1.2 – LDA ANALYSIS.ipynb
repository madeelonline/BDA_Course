{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priors: \n",
    "- Distribution over vocabulary for topic k in {1..K}: beta\\[k\\] ~ Dirichlet(V, eta)\n",
    "- Distribution over topics (latent variables): theta ~ Dirichlet(K, alpha)\n",
    "\n",
    "For each document:\n",
    "- Choose number of words: N ~ Poisson(ξ)\n",
    "\n",
    "For each of the N words w:\n",
    "- Choose a topic: z ~ Cat(K, theta)\n",
    "- Choose a word: w ~ Cat(V, beta\\[z\\])\n",
    "\n",
    "Note: This model follows the ‘bag of words’ assumption, such that given the topic proportions,\n",
    "each word drawn is independent of any other words in the document. \n",
    "\n",
    "![Graphical representation](images/LDA_PGM_representation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use variational inference, the edges between θ (theta), z and w are removed to make inference on LDA model tractable. \n",
    "\n",
    "![Variational Inference](images\\Variational_Distribution_representation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_url = 'https://www.eecs.mit.edu/people/faculty-advisors'\n",
    "arXiv_format = 'arxiv.org/find/{}/1/au:+{}_{}/0/1/0/all/0/1' # arxiv.org/find/(subject)/1/au:+(lastname)_(initial)/0/1/0/all/0/1\n",
    "search_url_format = 'https://arxiv.org/search/?query=\"{}\"&searchtype=author'\n",
    "subjects = {'Computer Science': 'Computer Science', \n",
    "            'Electrical Engineering': 'Electrical Engineering and Systems Science',\n",
    "            'Physics': 'Physics'}\n",
    "all_papers_columns = ['Name', 'Abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Sraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get Facultys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFacultyNames():\n",
    "    faculty_page = get(faculty_url)\n",
    "    faculty_page_content = BeautifulSoup(faculty_page.content, 'html.parser')\n",
    "    names_cont = faculty_page_content.select('div.views-field-title span.card-title a')\n",
    "    names = [name_cont.contents[0] for name_cont in names_cont]\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = getFacultyNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scrape Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeArXiV(names):\n",
    "    papers = list()\n",
    "    for name in names:\n",
    "        search_url = search_url_format.format(name.replace(' ', '+'))\n",
    "        papers_author = get(search_url)\n",
    "        papers_author_content = BeautifulSoup(papers_author.content, 'html.parser')\n",
    "        papers_author_body = papers_author_content.body\n",
    "        results = papers_author_body.find_all(\"li\", class_=\"arxiv-result\")\n",
    "        abstracts = [result.find(\"span\", class_=\"abstract-full\") for result in results]\n",
    "        \n",
    "        abstracts_content = [abstract.a.unwrap() for abstract in abstracts]\n",
    "        abstracts_content = [abstract.contents[0] for abstract in abstracts]\n",
    "\n",
    "        if abstracts_content:\n",
    "            papers = papers + abstracts_content\n",
    "        \n",
    "    return papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = scrapeArXiV(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cleaning_and_count(s):\n",
    "    s_lower = s.lower()\n",
    "    \n",
    "    cleaning_set = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(s_lower)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    word_dict = dict(collections.Counter(tokens))\n",
    "    for key in cleaning_set:\n",
    "        word_dict.pop(key, None)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_word_dict = [word_cleaning_and_count(paper) for paper in papers]\n",
    "dup_keys = []\n",
    "for i in range(len(papers_word_dict)):\n",
    "    dup_keys = dup_keys + list(papers_word_dict[i].keys())\n",
    "\n",
    "vocab = list(collections.Counter(dup_keys).keys())\n",
    "lookup_table = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/names', 'w') as fout:\n",
    "    json.dump(names, fout)\n",
    "with open('data/papers', 'w') as fout:\n",
    "    json.dump(papers, fout)\n",
    "with open('data/papers_word_dict', 'w') as fout:\n",
    "    json.dump(papers_word_dict, fout)\n",
    "with open('data/vocab', 'w') as fout:\n",
    "    json.dump(vocab, fout)\n",
    "with open('data/lookup_table', 'w') as fout:\n",
    "    json.dump(lookup_table, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/names', 'r') as json_file:\n",
    "    names = json.load(json_file)\n",
    "with open('data/papers', 'r') as json_file:\n",
    "    papers = json.load(json_file)\n",
    "with open('data/papers_word_dict', 'r') as json_file:\n",
    "    papers_word_dict = json.load(json_file)\n",
    "with open('data/vocab', 'r') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "with open('data/lookup_table', 'r') as json_file:\n",
    "    lookup_table = json.load(json_file)\n",
    "    \n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = []\n",
    "for paper in papers_word_dict: \n",
    "    doc_vec = [0 for _ in range(vocab_size)]\n",
    "    for token, occurs in paper.items(): \n",
    "        doc_vec[lookup_table[token]] = occurs\n",
    "    doc_vecs.append(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\data_science\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "information algorithms model results problem show paper network time based\n",
      "Topic 1:\n",
      "data model learning network neural networks using models performance training\n",
      "Topic 2:\n",
      "channel capacity csi coding also transmitter receiver scheme number length\n",
      "Topic 3:\n",
      "planning motion policy robot policies magnetic spin emission observations density\n",
      "Topic 4:\n",
      "n problem algorithm graph show time linear algorithms k results\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Run the LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, learning_method='online').fit(doc_vecs)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic %d:' % (topic_idx))\n",
    "        print(' '.join([vocab[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, doc_vecs, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end Code (SVILDA algorithm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = []\n",
    "for paper in papers_word_dict: \n",
    "    wordslist = []\n",
    "    countslist = []\n",
    "    for token, occurs in paper.items(): \n",
    "        wordslist.append(lookup_table[token])\n",
    "        countslist.append(occurs)\n",
    "    doc_vecs.append((wordslist, countslist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 0  running document number  161\n",
      "ITERATION 100  running document number  787\n",
      "ITERATION 200  running document number  369\n",
      "ITERATION 300  running document number  1031\n",
      "ITERATION 400  running document number  61\n",
      "ITERATION 500  running document number  884\n",
      "ITERATION 600  running document number  630\n",
      "ITERATION 700  running document number  1142\n",
      "ITERATION 800  running document number  982\n",
      "ITERATION 900  running document number  176\n",
      "ITERATION 1000  running document number  1493\n",
      "ITERATION 1100  running document number  826\n",
      "ITERATION 1200  running document number  495\n",
      "ITERATION 1300  running document number  1658\n",
      "ITERATION 1400  running document number  1016\n",
      "ITERATION 1500  running document number  708\n",
      "ITERATION 1600  running document number  591\n",
      "ITERATION 1700  running document number  438\n",
      "ITERATION 1800  running document number  1660\n",
      "ITERATION 1900  running document number  808\n",
      "ITERATION 2000  running document number  1217\n",
      "ITERATION 2100  running document number  1461\n",
      "ITERATION 2200  running document number  1288\n",
      "ITERATION 2300  running document number  137\n",
      "ITERATION 2400  running document number  1437\n",
      "ITERATION 2500  running document number  34\n",
      "ITERATION 2600  running document number  1389\n",
      "ITERATION 2700  running document number  30\n",
      "ITERATION 2800  running document number  1488\n",
      "ITERATION 2900  running document number  7\n",
      "ITERATION 3000  running document number  1778\n",
      "ITERATION 3100  running document number  829\n",
      "ITERATION 3200  running document number  1287\n",
      "ITERATION 3300  running document number  739\n",
      "ITERATION 3400  running document number  1486\n",
      "ITERATION 3500  running document number  1267\n",
      "ITERATION 3600  running document number  637\n",
      "ITERATION 3700  running document number  1075\n",
      "ITERATION 3800  running document number  620\n",
      "ITERATION 3900  running document number  91\n",
      "ITERATION 4000  running document number  324\n",
      "ITERATION 4100  running document number  704\n",
      "ITERATION 4200  running document number  1215\n",
      "ITERATION 4300  running document number  1388\n",
      "ITERATION 4400  running document number  1198\n",
      "ITERATION 4500  running document number  705\n",
      "ITERATION 4600  running document number  879\n",
      "ITERATION 4700  running document number  891\n",
      "ITERATION 4800  running document number  1609\n",
      "ITERATION 4900  running document number  645\n",
      "ITERATION 5000  running document number  1675\n",
      "ITERATION 5100  running document number  280\n",
      "ITERATION 5200  running document number  278\n",
      "ITERATION 5300  running document number  313\n",
      "ITERATION 5400  running document number  110\n",
      "ITERATION 5500  running document number  85\n",
      "ITERATION 5600  running document number  835\n",
      "ITERATION 5700  running document number  1101\n",
      "ITERATION 5800  running document number  1248\n",
      "ITERATION 5900  running document number  1720\n",
      "ITERATION 6000  running document number  17\n",
      "ITERATION 6100  running document number  1723\n",
      "ITERATION 6200  running document number  791\n",
      "ITERATION 6300  running document number  189\n",
      "ITERATION 6400  running document number  758\n",
      "ITERATION 6500  running document number  1254\n",
      "ITERATION 6600  running document number  1551\n",
      "ITERATION 6700  running document number  1515\n",
      "ITERATION 6800  running document number  1009\n",
      "ITERATION 6900  running document number  1038\n",
      "ITERATION 7000  running document number  1657\n",
      "ITERATION 7100  running document number  1676\n",
      "ITERATION 7200  running document number  1658\n",
      "ITERATION 7300  running document number  301\n",
      "ITERATION 7400  running document number  1795\n",
      "ITERATION 7500  running document number  1180\n",
      "ITERATION 7600  running document number  528\n",
      "ITERATION 7700  running document number  457\n",
      "ITERATION 7800  running document number  9\n",
      "ITERATION 7900  running document number  1362\n",
      "ITERATION 8000  running document number  786\n",
      "ITERATION 8100  running document number  119\n",
      "ITERATION 8200  running document number  1122\n",
      "ITERATION 8300  running document number  1359\n",
      "ITERATION 8400  running document number  133\n",
      "ITERATION 8500  running document number  1789\n",
      "ITERATION 8600  running document number  1016\n",
      "ITERATION 8700  running document number  74\n",
      "ITERATION 8800  running document number  65\n",
      "ITERATION 8900  running document number  1461\n",
      "ITERATION 9000  running document number  1095\n",
      "ITERATION 9100  running document number  1565\n",
      "ITERATION 9200  running document number  358\n",
      "ITERATION 9300  running document number  982\n",
      "ITERATION 9400  running document number  1172\n",
      "ITERATION 9500  running document number  254\n",
      "ITERATION 9600  running document number  976\n",
      "ITERATION 9700  running document number  142\n",
      "ITERATION 9800  running document number  926\n",
      "ITERATION 9900  running document number  965\n"
     ]
    }
   ],
   "source": [
    "from svilda import SVILDA\n",
    "iterations = 10000\n",
    "lda = SVILDA(vocab, no_topics, len(doc_vecs), 0.1, 0.01, 1, 0.75, iterations)\n",
    "lda.runSVI(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "paper number two problems information methods use systems provide large\n",
      "Topic 1:\n",
      "problem show n data also optimal networks performance propose channel\n",
      "Topic 2:\n",
      "model time network given one applications demonstrate function linear many\n",
      "Topic 3:\n",
      "algorithm algorithms new present approach based set work consider k\n",
      "Topic 4:\n",
      "results using method graph models learning system study result distribution\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model._lambda):\n",
    "        print('Topic %d:' % (topic_idx))\n",
    "        print(' '.join([vocab[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, doc_vecs, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
