{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitielib import *\n",
    "import numpy as np\n",
    "import utils\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBALS/CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_path='./articles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATHERING NEWS ARTICLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, it is assumed that you have access to electronic version of news stories as text, ideally all from a single day. It is suggested that you have at least 50 articles. You can extract the content of these articles, including their titles, manually or by using Python-based tools like PythonGoose (https://github.com/grangier/python-goose). \n",
    "\n",
    "The rest of this tutorial assumes that you have access to the content of each of the news articles in the following format: \n",
    " - <b>Filename:</b> title-<article_numer>.txt, example: title-1.txt\n",
    "* <b>Contents:</b> The title of the news story. \n",
    "\n",
    "\n",
    " - <b>Filename:</b> article-<article_numer>.txt, example: article-1.txt\n",
    "* <b>Contents:</b> The contents of the news story. \n",
    "        \n",
    "In addition, it is also recommended that you tag each article with the actual <b>“topic”</b> which will help us evaluate the performance of the spectral clustering algorithm. This can be done by assigning the name of the hierarchical identifier for the news story on the website it is hosted on. For instance, news stories about <b>“Brexit”</b> were typically classified under the <b>“Brexit”</b> section in most online news websites. Stories about the <b>“Middle East”</b> are typically classified under <b>“Middle East”</b>, and so on. If this information is available, it can be made available in the following format:\n",
    " - <b>Filename:</b> topic-<article_numer>.txt, example: topic-1.txt\n",
    "* <b>Contents:</b> The actual “topic” (section or sub-section) under which the news story was classified on the hosting website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_articles(articles_path, news_website = 'https://edition.cnn.com/world', language='en', lm_articles = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTITY EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to represent each article’s contents (corpus) as a “bag of entities”. This simply means that we will look for the mentions of certain words, i.e. names of people, organizations, locations etc. The master list of such entities can be found using various publicly available libraries. One such Python-based library is called “MITIE” (https://github.com/mit-nlp/MITIE). You will need to install it, and make a note about the path to the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER path   \n",
    "path_to_ner_model = './MITIE-models/english/ner_model.dat'\n",
    "ner = named_entity_extractor(path_to_ner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Articles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the articles from the function\n",
    "# total number of articles to process\n",
    "N = 300\n",
    "# in memory stores for the topics, titles and contents of the news stories\n",
    "topics_array = []\n",
    "titles_array = []\n",
    "corpus = []\n",
    "for i in range(N):\n",
    "    # get the contents of the article\n",
    "    with open(articles_path+f'/article-{i}.txt', 'r') as myfile:\n",
    "        d1=myfile.read().replace('\\n', '')\n",
    "        d1 = d1.lower()\n",
    "        corpus.append(d1)\n",
    "        \n",
    "    #get the original topic of the article\n",
    "    with open(articles_path+f'/topic-{i}.txt', 'r') as myfile:\n",
    "        to1=myfile.read().replace('\\n', '')\n",
    "        to1 = to1.lower()\n",
    "        topics_array.append(to1)\n",
    "        \n",
    "    #get the title of the article\n",
    "    with open(articles_path+f'/title-{i}.txt', 'r') as myfile:\n",
    "        ti1=myfile.read().replace('\\n', '')\n",
    "        ti1 = ti1.lower()\n",
    "        titles_array.append(ti1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As MITIE is installed, we are now ready to do the following:\n",
    "1. Loop over all the article text corpuses to determine all the unique words used across our dataset.\n",
    "2. Find the subset of the entities from the ner model that are among the unique words being used across the dataset (determined in step 1)\n",
    "\n",
    "This goal can be achieved using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'#vaccinescauseautism', b'&pizza', b\"'mona lisa\", ...,\n",
       "       b'zulkiflee anwar haque', b'zunar', b'zurcher'], dtype='|S62')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entity subset array\n",
    "entity_text_array = []\n",
    "for i in range(N):\n",
    "    # Load the article contents text file and convert it into a list of words.\n",
    "    tokens = tokenize(load_entire_file((articles_path+f'/article-{i}.txt')))\n",
    "    # extract all entities known to the ner model mentioned in this article\n",
    "    entities = ner.extract_entities(tokens)\n",
    "    # extract the actual entity words and append to the array\n",
    "    for e in entities:\n",
    "        range_array = e[0]\n",
    "        entity_text = b' '.join([tokens[j] for j in range_array])\n",
    "        entity_text_array.append(entity_text.lower())\n",
    "\n",
    "# remove duplicate entities detected\n",
    "entity_text_array = np.unique(entity_text_array)\n",
    "\n",
    "entity_text_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list of all entities used across our dataset, we can represent each article as a vector that contains the TF-IDF (https://en.wikipedia.org/wiki/Tf–idf ) score for each entity stored in the <i>entity_text_array</i>. This task can easily be achieved by using the scikit-learn library (http://scikitlearn.org/stable/) for Python. Please ensure scikit-learn is installed and ready to use before proceeding. The following lines of code can help represent each article in the dataset as a vector of TF-IDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct TfidVectorizer\n",
    "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',\n",
    "                       stop_words='english', vocabulary=entity_text_array)\n",
    "corpus_tf_idf = vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the articles represented as vectors of their TF-IDF scores, we are ready to perform <b>Spectral Clustering</b> on the articles. We can use the scikit-learn library for this purpose as well. The following lines of code will cluster our articles in to 7 clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 5\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : cnn.com - transcripts\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : this is a crucial week for the us-china trade war\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 5\n",
      "Document Title : chinese woman who paid $6.5m after her daughter got into stanford says she did it to help others\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : second canadian sentenced to death in china for drug smuggling\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 0\n",
      "Document Title : six marxist students vanish in china in the lead up to labor day\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : us trade deficit edges up, even as americans buy less from china\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 0\n",
      "Document Title : trump heats up yet another global crisis by escalating china trade war\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : the woman teaching african companies the art of closing deals with the chinese\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : north korea is waiting for trump to blink ... or leave office\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : destructive pest could spread to all of china's grain production in 12 months\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : tariffs loom as us-china talks begin\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : facing an aggressive beijing, taiwan's president issues a warning to the world\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : the stories behind 10 of the world's earliest photographs\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : a 'love match' that won over japan\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : a \"love match\" that won over japan\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : thailand bay made popular by 'the beach' to remain closed for two more years\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : japanese town asks tourists not to eat while walking\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : amazing destinations with almost no tourists\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 4\n",
      "Document Title : how cricket gives street children a voice\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : kaito toba: japan's rising star could be savior\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : horse racing calendar 2019: the schedule\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : china's transgender people driven to self-medicate, report says\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : pakistan's anti-vaccination movement leads to string of deadly attacks\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : scientists with ties to china ousted from us cancer center amid fears of foreign influence\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : former xinjiang teacher claims brainwashing and abuse inside mass detention centers\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : the alarming assault on the free press\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : trump, the deal breaker, aims for a grand bargain\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : what sri lanka needs now\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : north korea launches 'two short-range missiles'\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 2\n",
      "Document Title : rare black bear spotted in demilitarized zone between north and south korea\n",
      "------------------------\n",
      "Document Topic : edition.cnn.com\n",
      "Cluster Assignment : 3\n",
      "Document Title : security cameras and barbed wire: living amid fear and oppression in xinjiang\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# change n_clusters to equal the number of clusters desired\n",
    "n_clusters = 7\n",
    "#spectral clustering\n",
    "spectral = cluster.SpectralClustering(n_clusters= n_clusters,\n",
    "                                      eigen_solver='arpack',\n",
    "                                      affinity=\"nearest_neighbors\",\n",
    "                                      n_neighbors = 17)\n",
    "spectral.fit(corpus_tf_idf)\n",
    "\n",
    "if hasattr(spectral, 'labels_'):\n",
    "    cluster_assignments = spectral.labels_.astype(np.int)\n",
    "for i in range(0, 40): #len(cluster_assignments))\n",
    "    # Topics of documents doesn't make sense because the site (cnn)\n",
    "    # didn't have very good topics by default\n",
    "    print('Document Topic : {}'.format(topics_array[i]))\n",
    "    print('Cluster Assignment : {}'.format(cluster_assignments[i]))\n",
    "    print('Document Title : {}'.format(titles_array[i]))\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table doesn't make sense because the site (cnn) didn't have very good topics by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"7\" halign=\"left\">val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assignment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>edition.cnn.com</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>112</td>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money.cnn.com</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.abc12.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.cbs46.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.cnn.com</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.kctv5.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.kptv.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www.wsmv.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                val                       \n",
       "Assignment        0   1    2    3  4  5  6\n",
       "Topic                                     \n",
       "edition.cnn.com   9   5  112  105  3  3  6\n",
       "money.cnn.com     1  12   16   15  1  0  2\n",
       "www.abc12.com     0   0    1    0  0  0  0\n",
       "www.cbs46.com     0   0    0    1  0  0  0\n",
       "www.cnn.com       1   0    2    1  0  1  0\n",
       "www.kctv5.com     0   0    1    0  0  0  0\n",
       "www.kptv.com      0   0    0    1  0  0  0\n",
       "www.wsmv.com      0   0    0    0  0  0  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Topic': topics_array, 'Assignment': cluster_assignments})\n",
    "df['val'] = 1\n",
    "tb = df.groupby(['Topic', 'Assignment']).sum().unstack().fillna(0).astype(int)\n",
    "tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
